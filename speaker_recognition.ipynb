{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet.resnet_se_34v2 import ResNetSE34V2\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "from torchaudio.functional import amplitude_to_DB\n",
    "import torch\n",
    "import torchaudio\n",
    "import yaml\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all path from the audio directory and save it to a list\n",
    "def get_all_audio_path(audio_dir):\n",
    "    audio_path = []\n",
    "    for root, dirs, files in os.walk(audio_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_path.append(os.path.join(root, file))\n",
    "    return audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = get_all_audio_path(\"audio/speaker_segments/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file):\n",
    "    EPS = 1e-8\n",
    "    s, _ = librosa.load(file, sr=16000)\n",
    "    amax = np.max(np.abs(s))\n",
    "    factor = 1.0 / (amax + EPS)\n",
    "    s = s * factor\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 256, encoder SAP.\n"
     ]
    }
   ],
   "source": [
    "with open('./models/resnet/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "sd = torch.load('./models/weigths/resnetse34_epoch92_eer0.00931.pth')\n",
    "model = ResNetSE34V2(nOut=256, n_mels= 80)\n",
    "model.load_state_dict(sd)\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "transform = MelSpectrogram(\n",
    "    sample_rate= 16000,\n",
    "    n_fft= 512,\n",
    "    win_length= 400,\n",
    "    hop_length= 160,\n",
    "    window_fn=torch.hamming_window,\n",
    "    n_mels= 80,\n",
    "    f_min=20,\n",
    "    f_max=7600,\n",
    "    norm='slaney')\n",
    "\n",
    "\n",
    "def embed_inference(audio_path, transform= transform, model = model):\n",
    "    s = load_audio(audio_path)\n",
    "    x = torch.tensor(s[None, :])\n",
    "    x = transform(x)\n",
    "    x = amplitude_to_DB(\n",
    "        x, multiplier=10, amin= 1e-5, db_multiplier=0, top_db=75)\n",
    "\n",
    "    feature = model(x[:, None, :, :])\n",
    "    feature = torch.nn.functional.normalize(feature)\n",
    "    return(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CosineSimilarity(dim=1, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:54<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "audio_paths_compar = audio_paths[1:]\n",
    "ref_embed = embed_inference(audio_paths[0])\n",
    "\n",
    "compar_tab = pd.DataFrame(columns = ['path_1', 'path_2', 'label', 'loss_man', 'loss_val'])\n",
    "with tqdm(total=len(audio_paths_compar)) as pbar:\n",
    "    for path_1 in audio_paths_compar :\n",
    "        spk_1 = path_1.split('/')[-1][15]\n",
    "        ref_embed = embed_inference(path_1)\n",
    "        for path_2 in audio_paths_compar :\n",
    "            spk_2 = path_2.split('/')[-1][15]\n",
    "            if spk_1 == spk_2 :\n",
    "                label = 1        \n",
    "            else :\n",
    "                label = 0\n",
    "\n",
    "            \n",
    "            embed = embed_inference(path_2)\n",
    "            loss_val = loss(ref_embed, embed)\n",
    "            #print(embed)\n",
    "            #print(ref_embed)\n",
    "            val = torch.dot(ref_embed.squeeze(0), embed.squeeze(0))\n",
    "            compar_tab.loc[len(compar_tab)] = [path_1, path_2, label, val, loss_val]\n",
    "            #print(label, val)\n",
    "        pbar.update(1)\n",
    "       \n",
    "\n",
    "           \n",
    "compar_tab.to_csv('compar_tab.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_paths_compar = audio_paths[1:]\n",
    "ref_embed = embed_inference(audio_paths[0])\n",
    "\n",
    "compar_tab = pd.DataFrame(columns = ['path_1', 'path_2', 'score'])\n",
    "for path_1 in audio_paths_compar :\n",
    "    ref_embed = embed_inference(path_1)\n",
    "    for path_2 in audio_paths_compar :\n",
    "        embed = embed_inference(path_2)\n",
    "        #print(embed)\n",
    "        #print(ref_embed)\n",
    "        loss_value = loss(ref_embed, embed)\n",
    "       \n",
    "\n",
    "        compar_tab.loc[len(compar_tab)] = [path_1, path_2, loss_value.item()]\n",
    "        compar_tab.to_csv('compar_tab.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.467571496963501\n"
     ]
    }
   ],
   "source": [
    "print(test.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
