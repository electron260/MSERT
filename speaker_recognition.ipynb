{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet.resnet_se_34v2 import ResNetSE34V2\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "from torchaudio.functional import amplitude_to_DB\n",
    "import torch\n",
    "import torchaudio\n",
    "import yaml\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all path from the audio directory and save it to a list\n",
    "def get_all_audio_path(audio_dir):\n",
    "    audio_path = []\n",
    "    for root, dirs, files in os.walk(audio_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_path.append(os.path.join(root, file))\n",
    "    return audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = get_all_audio_path(\"audio/speaker_segments/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file):\n",
    "    EPS = 1e-8\n",
    "    s, _ = librosa.load(file, sr=16000)\n",
    "    amax = np.max(np.abs(s))\n",
    "    factor = 1.0 / (amax + EPS)\n",
    "    s = s * factor\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 256, encoder SAP.\n"
     ]
    }
   ],
   "source": [
    "with open('./models/resnet/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "sd = torch.load('./models/weigths/resnetse34_epoch92_eer0.00931.pth')\n",
    "model = ResNetSE34V2(nOut=256, n_mels=config['fbank']['n_mels'])\n",
    "model.load_state_dict(sd)\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "transform = MelSpectrogram(\n",
    "    sample_rate=config['fbank']['sr'],\n",
    "    n_fft=config['fbank']['n_fft'],\n",
    "    win_length=config['fbank']['win_length'],\n",
    "    hop_length=config['fbank']['hop_length'],\n",
    "    window_fn=torch.hamming_window,\n",
    "    n_mels=config['fbank']['n_mels'],\n",
    "    f_min=config['fbank']['f_min'],\n",
    "    f_max=config['fbank']['f_max'],\n",
    "    norm='slaney')\n",
    "\n",
    "\n",
    "def embed_inference(audio_path, transform= transform, model = model):\n",
    "    s = load_audio(audio_path)\n",
    "    x = torch.tensor(s[None, :])\n",
    "    x = transform(x)\n",
    "    x = amplitude_to_DB(\n",
    "        x, multiplier=10, amin=config['fbank']['amin'], db_multiplier=0, top_db=75)\n",
    "\n",
    "    feature = model(x[:, None, :, :])\n",
    "    feature = torch.nn.functional.normalize(feature)\n",
    "    return(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CosineSimilarity(dim=1, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_paths_compar = audio_paths[1:]\n",
    "ref_embed = embed_inference(audio_paths[0])\n",
    "\n",
    "compar_tab = pd.DataFrame(columns = ['path_1', 'path_2', 'score'])\n",
    "for path_1 in audio_paths_compar :\n",
    "    ref_embed = embed_inference(path_1)\n",
    "    for path_2 in audio_paths_compar :\n",
    "        embed = embed_inference(path_2)\n",
    "        #print(embed)\n",
    "        #print(ref_embed)\n",
    "        loss_value = loss(ref_embed, embed)\n",
    "       \n",
    "\n",
    "        compar_tab.loc[len(compar_tab)] = [path_1, path_2, loss_value.item()]\n",
    "        compar_tab.to_csv('compar_tab.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.467571496963501\n"
     ]
    }
   ],
   "source": [
    "print(test.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
