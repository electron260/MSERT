{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Install dependencies\n",
    "! pip install wget\n",
    "! apt-get install sox libsndfile1 ffmpeg\n",
    "! pip install text-unidecode\n",
    "\n",
    "# ## Install NeMo\n",
    "BRANCH = 'r1.20.0'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
    "\n",
    "## Install TorchAudio\n",
    "! pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'XXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper import load_model\n",
    "\n",
    "# Large models result in considerably better and more aligned (words, timestamps) mapping. \n",
    "model = load_model(\"large-v2\")\n",
    "\n",
    "# Beam size if None by default (Greedy Decoding). You can also set the\n",
    "# beam_size to some number like 5. This will increase in better transcription\n",
    "# quality but it'll increase runtime considerabley.\n",
    "results = model.transcribe('./audio_16k.wav', beam_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "\n",
    "device = 'cuda'\n",
    "model_a, metadata = whisperx.load_align_model(language_code=results[\"language\"], device=device)\n",
    "whisperx.align(results[\"segments\"], model_a, metadata, './audio_16k.wav', device, 0.0, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You WhisperX alignment may fail. If this happens then it's most probably because Whisper just hallucinated i.e. whisper came up with extra/weird output at the end.\n",
    "\n",
    "This usually happens with long audio files. If this happens, I'd suggest splitting big audio files in small files.\n",
    "\n",
    "# Storing words <> timestamps mapping in a file.\n",
    "\n",
    "import json\n",
    "\n",
    "with open('./word_ts.text', 'w+') as f:\n",
    "    for result in results['segments']:\n",
    "      for line in result['word-level']:\n",
    "        line_temp = line.copy()\n",
    "        # WhisperX don't put a space after word but just to make sure.\n",
    "        line_temp['text'] = line_temp['text'].strip()\n",
    "        f.write(f'{json.dumps(line_temp)}\\n')\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.parts.utils.diarization_utils import OfflineDiarWithASR\n",
    "from nemo.collections.asr.models.msdd_models import ClusteringDiarizer\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import librosa\n",
    "import os\n",
    "import wget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nemo\n",
    "import glob\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import shutil\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "signal, sr = librosa.load(audio_path,sr=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_waveform(signal,text='Audio',overlay_color=[]):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    fig.set_figwidth(20)\n",
    "    fig.set_figheight(2)\n",
    "    plt.scatter(np.arange(len(signal)),signal,s=1,marker='o',c='k')\n",
    "    if len(overlay_color):\n",
    "        plt.scatter(np.arange(len(signal)),signal,s=1,marker='o',c=overlay_color)\n",
    "    fig.suptitle(text, fontsize=16)\n",
    "    plt.xlabel('time (secs)', fontsize=18)\n",
    "    plt.ylabel('signal strength', fontsize=14);\n",
    "    plt.axis([0,len(signal),-0.5,+0.5])\n",
    "    time_axis,_ = plt.xticks();\n",
    "    plt.xticks(time_axis[:-1],time_axis[:-1]/sample_rate);\n",
    "    \n",
    "COLORS=\"b g c m y\".split()\n",
    "\n",
    "def get_color(signal,speech_labels,sample_rate=16000):\n",
    "    c=np.array(['k']*len(signal))\n",
    "    for time_stamp in speech_labels:\n",
    "        start,end,label=time_stamp.split()\n",
    "        start,end = int(float(start)*16000),int(float(end)*16000),\n",
    "        if label == \"speech\":\n",
    "            code = 'red'\n",
    "        else:\n",
    "            code = COLORS[int(label.split('_')[-1])]\n",
    "        c[start:end]=code\n",
    "    \n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_waveform(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for NeMo Speaker Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DOMAIN_TYPE = 'meeting'\n",
    "\n",
    "CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
    "CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "data_dir = os.path.join(ROOT,'data')\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir,CONFIG_FILE_NAME)):\n",
    "    CONFIG = wget.download(CONFIG_URL, data_dir)\n",
    "else:\n",
    "    CONFIG = os.path.join(data_dir,CONFIG_FILE_NAME)\n",
    "\n",
    "cfg = OmegaConf.load(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "meta = {\n",
    "    'audio_filepath': audio_path, \n",
    "    'offset': 0, \n",
    "    'duration':None, \n",
    "    'label': 'infer', \n",
    "    'text': '-', \n",
    "    'num_speakers': None, \n",
    "    'rttm_filepath': None, \n",
    "    'uem_filepath' : None\n",
    "}\n",
    "\n",
    "with open(os.path.join(data_dir,'input_manifest.json'),'w') as fp:\n",
    "    json.dump(meta,fp)\n",
    "    fp.write('\\n')\n",
    "\n",
    "cfg.diarizer.manifest_filepath = os.path.join(data_dir,'input_manifest.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.num_workers = 4\n",
    "cfg.batch_size = 32\n",
    "\n",
    "cfg.diarizer.speaker_embeddings.parameters.window_length_in_sec = [1.5, 1.0, 0.5]\n",
    "cfg.diarizer.speaker_embeddings.parameters.shift_length_in_sec = [0.75, 0.5, 0.25]\n",
    "cfg.diarizer.speaker_embeddings.parameters.multiscale_weights = [0.33, 0.33, 0.33]\n",
    "\n",
    "pretrained_speaker_model='titanet_large'\n",
    "cfg.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n",
    "cfg.diarizer.clustering.parameters.oracle_num_speakers=False\n",
    "cfg.diarizer.out_dir = data_dir\n",
    "\n",
    "cfg.diarizer.ignore_overlap = False\n",
    "cfg.diarizer.oracle_vad = False\n",
    "cfg.diarizer.collar = 0.25\n",
    "\n",
    "cfg.diarizer.vad.model_path = 'vad_multilingual_marblenet'\n",
    "cfg.diarizer.oracle_vad = False # ----> Not using oracle VAD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asr_diar_offline = OfflineDiarWithASR(cfg.diarizer)\n",
    "model = ClusteringDiarizer(cfg=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.diarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
